{"transformers.tokenization_xlnet.XLNetTokenizer.__init__": ["transformers.tokenization_utils.PreTrainedTokenizer.__init__", "<builtin>.super", "sentencepiece.SentencePieceProcessor"], "transformers.tokenization_bert.BertTokenizer.__init__": ["transformers.tokenization_utils.PreTrainedTokenizer.__init__", "<builtin>.super", "os.path.isfile", "<builtin>.ValueError", "transformers.tokenization_bert.load_vocab", "collections.OrderedDict", "transformers.tokenization_bert.BasicTokenizer.__init__", "transformers.tokenization_bert.WordpieceTokenizer.__init__"], "transformers.modeling_utils.SequenceSummary.__init__": ["<builtin>.super", "<builtin>.getattr", "transformers.modeling_utils.Identity.__init__", "torch.nn.Identity", "<builtin>.hasattr", "torch.nn.Linear", "transformers.activations.get_activation", "torch.nn.Dropout"], "transformers.tokenization_utils.PreTrainedTokenizer.__init__": ["<builtin>.int", "<builtin>.set", "<builtin>.setattr", "<builtin>.isinstance", "<builtin>.all"], "transformers.tokenization_bert.load_vocab": ["collections.OrderedDict", "<builtin>.open", "<builtin>.enumerate"], "transformers.modeling_utils.Identity.__init__": ["<builtin>.super"], "transformers.activations.get_activation": ["<builtin>.KeyError", "<builtin>.list"]}