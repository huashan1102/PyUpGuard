{"transformers.optimization.AdamW.__init__": ["<builtin>.ValueError", "<builtin>.dict", "torch.optim.Optimizer.__init__", "<builtin>.super"], "transformers.optimization.get_linear_schedule_with_warmup": ["<builtin>.max", "<builtin>.float", "torch.optim.lr_scheduler.LambdaLR"], "transformers.modeling_bert.BertModel.__init__": ["transformers.modeling_utils.PreTrainedModel.__init__", "<builtin>.super", "transformers.modeling_bert.BertEmbeddings.__init__", "transformers.modeling_bert.BertEncoder.__init__", "transformers.modeling_bert.BertPooler.__init__", "transformers.modeling_utils.PreTrainedModel.init_weights"], "transformers.modeling_utils.PreTrainedModel.__init__": ["<builtin>.super", "<builtin>.isinstance", "<builtin>.ValueError"], "transformers.modeling_bert.BertEmbeddings.__init__": ["<builtin>.super", "torch.nn.Embedding", "torch.nn.LayerNorm", "torch.nn.Dropout", "torch.arange"], "transformers.modeling_bert.BertEncoder.__init__": ["<builtin>.super", "torch.nn.ModuleList", "transformers.modeling_bert.BertLayer.__init__", "<builtin>.range"], "transformers.modeling_bert.BertPooler.__init__": ["<builtin>.super", "torch.nn.Linear", "torch.nn.Tanh"], "transformers.modeling_utils.PreTrainedModel.init_weights": ["transformers.modeling_utils.PreTrainedModel.prune_heads", "transformers.modeling_utils.PreTrainedModel.tie_weights"], "transformers.modeling_bert.BertLayer.__init__": ["<builtin>.super", "transformers.modeling_bert.BertAttention.__init__", "transformers.modeling_bert.BertIntermediate.__init__", "transformers.modeling_bert.BertOutput.__init__"], "transformers.modeling_utils.PreTrainedModel.prune_heads": ["<builtin>.list", "<builtin>.set"], "transformers.modeling_utils.PreTrainedModel.tie_weights": ["transformers.modeling_utils.PreTrainedModel.get_output_embeddings", "transformers.modeling_utils.PreTrainedModel._tie_or_clone_weights", "transformers.modeling_utils.PreTrainedModel.get_input_embeddings", "transformers.modeling_utils.PreTrainedModel._tie_encoder_decoder_weights"], "transformers.modeling_bert.BertAttention.__init__": ["<builtin>.super", "transformers.modeling_bert.BertSelfAttention.__init__", "transformers.modeling_bert.BertSelfOutput.__init__", "<builtin>.set"], "transformers.modeling_bert.BertIntermediate.__init__": ["<builtin>.super", "torch.nn.Linear", "<builtin>.isinstance"], "transformers.modeling_bert.BertOutput.__init__": ["<builtin>.super", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.Dropout"], "transformers.modeling_utils.PreTrainedModel._tie_or_clone_weights": ["torch.nn.Parameter", "<builtin>.getattr", "torch.nn.functional.pad", "<builtin>.hasattr"], "transformers.modeling_utils.PreTrainedModel.get_input_embeddings": ["<builtin>.getattr"], "transformers.modeling_utils.PreTrainedModel._tie_encoder_decoder_weights": ["<builtin>.hasattr", "<builtin>.isinstance", "<builtin>.len", "<builtin>.set", "<builtin>.list", "<builtin>.str", "<builtin>.int", "<builtin>.type", "<builtin>.ValueError"], "transformers.modeling_bert.BertSelfAttention.__init__": ["<builtin>.super", "<builtin>.ValueError", "<builtin>.hasattr", "<builtin>.int", "torch.nn.Linear", "torch.nn.Dropout"], "transformers.modeling_bert.BertSelfOutput.__init__": ["<builtin>.super", "torch.nn.Linear", "torch.nn.LayerNorm", "torch.nn.Dropout"]}